{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "650M8bdclfyx"
      },
      "source": [
        "# Reference \n",
        "https://huggingface.co/joeddav/bart-large-mnli-yahoo-answers\n",
        "https://huggingface.co/facebook/bart-large-mnli"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AIsszm0uJapb"
      },
      "outputs": [],
      "source": [
        "!pip install datasets\n",
        "!pip install transformers\n",
        "!pip install -U scikit-learn\n",
        "!pip install sentencepiece\n",
        "!pip install openpyxl\n",
        "!pip install pyvi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gdown"
      ],
      "metadata": {
        "id": "_y3eW3Z8MdqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown https://drive.google.com/uc?id=1PpR36GRQR26nRysMkgOz2S870dspBMyY  # for files\n",
        "# gdown <file_id>                                 # alternative format\n",
        "# gdown --folder https://drive.google.com/drive/folders/<file_id>  # for folders\n",
        "# gdown --folder --id <file_id>                                   # this format works for folders too"
      ],
      "metadata": {
        "id": "zjvNgbqc61IO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yNhTDwyOq6qI"
      },
      "outputs": [],
      "source": [
        "!ls -la"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6j_lBPCrNuI"
      },
      "outputs": [],
      "source": [
        "!unzip \"/content/data.zip\" -d \"/content/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fzD8Bd1jqISY"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import string\n",
        "\n",
        "import pandas as pd\n",
        "from pyvi import ViTokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def remove_stopwords(tokens):\n",
        "    stopwords = open(\"stopwords.txt\").read().split(\"\\n\")\n",
        "    tokens = [i.replace(\"_\", \" \") for i in tokens]\n",
        "    tokens = [i for i in tokens if i not in stopwords]\n",
        "    text = \" \".join(i for i in tokens)\n",
        "    return text\n",
        "\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    text = text.lower()\n",
        "    table = str.maketrans({key: None for key in string.punctuation})\n",
        "    text = text.translate(table)\n",
        "    text = re.sub('\\s+', ' ', text)\n",
        "    return text\n",
        "\n",
        "\n",
        "def normalize(text):\n",
        "    # tokens = ViTokenizer.tokenize(text).split()\n",
        "    tokens = ViTokenizer.tokenize(text)\n",
        "    text = tokens\n",
        "    # text = remove_stopwords(tokens)\n",
        "    # text = remove_punctuation(text)\n",
        "    return text\n",
        "from datasets import Dataset\n",
        "\n",
        "def load_dataset(path):\n",
        "    df = pd.read_excel(path)\n",
        "    X = list(df[\"text\"])\n",
        "    X = [normalize(x) for x in X]\n",
        "    # y = df.drop(\"text\", axis=1)\n",
        "    # columns = y.columns\n",
        "    # temp = y.apply(lambda item: item > 0)\n",
        "    # y = list(temp.apply(lambda item: list(columns[item.values]), axis=1))\n",
        "    # return X, y\n",
        "    df['text']=X\n",
        "    y = df.drop(\"text\", axis=1)\n",
        "    columns = y.columns\n",
        "    temp = y.apply(lambda item: item > 0)\n",
        "    column_labels = df.columns.drop('text')\n",
        "    df['label']=y.apply(lambda row: list(row.values), axis=1)\n",
        "    # df['label_text']=list(temp.apply(lambda item: list(columns[item.values]), axis=1))\n",
        "\n",
        "    dataset = Dataset.from_pandas(df.drop(column_labels, axis=1))\n",
        "    dataset=dataset.train_test_split(test_size=0.2)\n",
        "    return dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0yEkhgWMqtdg"
      },
      "outputs": [],
      "source": [
        "import datasets\n",
        "import torch\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "import pathlib\n",
        "import os\n",
        "from datasets import Dataset\n",
        "\n",
        "\n",
        "\n",
        "# https://huggingface.co/docs/transformers/tasks/token_classification\n",
        "#BARTpho-word\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bartpho-word\")\n",
        "bartpho_word = AutoModel.from_pretrained(\"vinai/bartpho-word\")\n",
        "TXT = 'Chúng tôi là những nghiên cứu viên.'\n",
        "input_ids = tokenizer(TXT, truncation=True)\n",
        "input_ids = tokenizer(TXT, return_tensors='pt')\n",
        "# print(TXT, input_ids)\n",
        "# input_ids = tokenizer(TXT, is_split_into_words=True)\n",
        "# tokens = tokenizer.convert_ids_to_tokens(input_ids['input_ids'])\n",
        "# print(tokens)\n",
        "features = bartpho_word(input_ids['input_ids'])\n",
        "# print(features)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bartpho_word"
      ],
      "metadata": {
        "id": "f73xMy1Rn05Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F3cEFNNjkV46"
      },
      "outputs": [],
      "source": [
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=True,truncation=True, max_length=512 )\n",
        "# , return_tensors='pt', max_length=512, truncation=True\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train_path = os.path.join(pathlib.Path(__file__).parent.parent.resolve(), 'train.xlsx')\n",
        "# print(\"Load data\", train_path)\n",
        "# dataset = load_dataset(train_path)\n",
        "# dataset.save_to_disk(\"../data/data_train\")\n",
        "dataset = datasets.load_from_disk(\"/content/data/data_train\")"
      ],
      "metadata": {
        "id": "cFYO_wwJ_Qst"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['train']"
      ],
      "metadata": {
        "id": "rw4QtSub_1pb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "Zwq7_UjI_VIh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
        "print('dataset', tokenized_dataset['train'][0])\n",
        "tokenized_dataset"
      ],
      "metadata": {
        "id": "o0HD-drz_Snk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset['train']"
      ],
      "metadata": {
        "id": "57x7SPR3-i3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokenized_dataset['train']['input_ids'][0])"
      ],
      "metadata": {
        "id": "MyV3S08_m-kk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6OT2A6b8kYtT"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorWithPadding\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "print('data_collator sample')\n",
        "# tokenized_dataset = tokenized_dataset.remove_columns(dataset[\"train\"].column_names)\n",
        "# tokenized_dataset = tokenized_dataset.remove_columns(dataset[\"test\"].column_names)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokenized_dataset[\"train\"])"
      ],
      "metadata": {
        "id": "wItaeQgCCM1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PIeOBswrkbJ4"
      },
      "outputs": [],
      "source": [
        "print('data loaded....................')\n",
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer, MBartForSequenceClassification\n",
        "model = MBartForSequenceClassification.from_pretrained(\"vinai/bartpho-word\", num_labels=10)\n",
        "# model = AutoModelForSequenceClassification.from_pretrained(\"vinai/bartpho-word\", num_labels=10)\n",
        "\n",
        "print('load model AutoModelForSequenceClassification')\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=5,\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "print('load model Trainner')\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset[\"train\"],\n",
        "    eval_dataset=tokenized_dataset[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "print('load model trainer.train()')\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model(\"bart_class_train\")"
      ],
      "metadata": {
        "id": "tPUmEcOz8e7L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################################################################"
      ],
      "metadata": {
        "id": "WPGGoWGKnIip"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "tranformers_bartpho.ipynb",
      "provenance": [],
      "private_outputs": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}